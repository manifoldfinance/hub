[{"id":0,"href":"/hub/docs/openmev/","title":"OpenMEV Documentation","section":"Docs","content":"OpenMEV Platform Documentation #   Strategy and Implementation details End User information Help Desk and Troubleshooting Searcher Integration Formulas and Proofs Technical: Technical overview on specific category  SDK #  NodeJS Packaging\n @openmev/ethers-provider use-react-wallet  react-hook with zustand state mgmt for providing wallet connectivity to OpenMEV RPC Endpoints   @openmev/sdk  Do you require integration in a different programming language? Reach out to us and we can help facilitate a solution for you\nSushiSwap #  The SushiSwap integration provides a service that realizes profit by transaction batching for the purposes of arbitrage by controlling transaction ordering.\nRight now every user sends a transaction directly to the network mempool and thus give away the arbitrage, front-running, back-running opportunities to miners(or random bots).\nOpenMEV provides a credibly neutral platform that enables aggregation of transactions (batching) for the purposes of extracting MEV profits and returning them back to the traders.\nWhat is credible neutrality? #   \u0026ldquo;\u0026hellip;that it is not just neutrality that is required here, it is credible neutrality. That is, it is not just enough for a mechanism to not be designed to favor specific people or outcomes over others; it’s also crucially important for a mechanism to be able to convince a large and diverse group of people that the mechanism at least makes that basic effort to be fair.\u0026rdquo;\n Vitalik Buterin, credible neutrality as a guiding principle   This ethos is at the heart of OpenMEV. Part of establishing credible neutrality is having a clear and comprehensive rulebook that regulates off-chain behavior and activities. Our assumption concerning governance is that methods and processes that work in legacy markets may not be applicable in adversarial environments such as permissionless blockchains. With that understanding it is important not to rely solely on such systems and mechanics long term.\nDiscuss this and more on our discourse forums\n"},{"id":1,"href":"/hub/docs/openmev/rebate/","title":"Rebating Transaction Costs","section":"OpenMEV Documentation","content":"Rebating Transaction Costs #  Rebating a transaction is determined by:\n  Is the function that is called in the transaction eligible?\n By tracking contract function calls we are better able to provide observability in the rebating process, we can also coordinate with teams wishing to provide more incentives for specific actions and behaviors    If yes, what is the percentage that can be rebated?\n This percentage value is a protocol value that can be adjusted    What is the transaction cost for the eligible transaction?\n Thi s the value that the end user utilized in submitting their transaction.    Calculate the Gas Reporting Index value\n This uses the gas pricing information from api.txprice.com to calculate the gas pricing information to be used in calculating the rebate amount for your transaction    Calculate the rebate amount from the bundle profit surplus\n We take how much profit the arbitrage made and split it among all eligible trades within that bundle    Rebate Mechanism #    Eligible transactions are rebated based on the 80th confidence interval for gas estimation pricing.\n  This is proportionally distributed based on transactional weight. Note: a naive formula would consider pairings, then slippage tolerance and finally transactional amount\n  The amount of compensation is the remainderless fees to miners and network operational transactional costs.\n  Compensation payouts occur no later than a half hour\n  Contract Function Eligibility #     $function_calls %eligible     swapExactTokensForTokens 100   swapExactTokensForETH 100   swapExactETHForTokens 100   swapETHForExactTokens 100   getAmountsOut null   addLiquidityETH 50   addLiquidity 50   swapTokensForExactTokens 100   getAmountOut null   removeLiquidityETHWithPermit 100   swapTokensForExactETH 100   removeLiquidityWithPermit 25   removeLiquidityETH 25   removeLiquidity 25   factory null   swapExactTokensForETHSupportingFeeOnTransferTokens #   swapExactTokensForTokensSupportingFeeOnTransferTokens #   getAmountsIn null   WETH null   swapExactETHForTokensSupportingFeeOnTransferTokens #   getAmountIn null   removeLiquidityETHWithPermitSupportingFeeOnTransferTokens #   removeLiquidityETHSupportingFeeOnTransferTokens #    Rebate Calculations #  Note: naive implementation, expect changes\nbundleCost = mev bribe + bundleTxs[1,2,\u0026hellip;]\ngasAllowance = mev bribe - bundleTxs[1,2,\u0026hellip;]\nBundleTransactionGas[1,2,\u0026hellip;] = Individual Gas Cost\nBundleId = The Block Number (or hash?) in which the bundle was included\nmax_gasRebate = (BundleId(BundleTransactionGas[1,2,\u0026hellip;]))\n{ \u0026#34;confidence\u0026#34;: 80, \u0026#34;price\u0026#34;: 150, \u0026#34;maxPriorityFeePerGas\u0026#34;: 1.75, \u0026#34;maxFeePerGas\u0026#34;: 100 } Workflow Diagram #  "},{"id":2,"href":"/hub/docs/openmev/functions_table/","title":"SushiSwap Router Function Calls","section":"OpenMEV Documentation","content":"SushiSwap Router Function Calls #  Router Function Calls #  0xd9e1cE17f2641f24aE83637ab66a2cca9C378B9F    $function_calls %eligible     swapExactTokensForTokens 100   swapExactTokensForETH 100   swapExactETHForTokens 100   swapETHForExactTokens 100   getAmountsOut null   addLiquidityETH 50   addLiquidity 50   swapTokensForExactTokens 100   getAmountOut null   removeLiquidityETHWithPermit 100   swapTokensForExactETH 100   removeLiquidityWithPermit 25   removeLiquidityETH 25   removeLiquidity 25   factory null   swapExactTokensForETHSupportingFeeOnTransferTokens #   swapExactTokensForTokensSupportingFeeOnTransferTokens #   getAmountsIn null   WETH null   swapExactETHForTokensSupportingFeeOnTransferTokens #   getAmountIn null   removeLiquidityETHWithPermitSupportingFeeOnTransferTokens #   removeLiquidityETHSupportingFeeOnTransferTokens #    "},{"id":3,"href":"/hub/posts/inflation_german/","title":"German Hyperinflation","section":"Blog","content":"Hyperinflation in the Weimar Republic #  Finally, a brief word on a favorite example of advocates of private control over money issuance, the German hyperinflation of 1923, which was supposedly caused by excessive government money printing. The Reichsbank president at the time, Hjalmar Schacht, putthe record straight on the real causes of that episode in Schacht (1967). Specifically, in May 1922 the Allies insisted on granting total private control over the Reichsbank. Thisprivate institution then allowed private banks to issue massive amounts of currency, until half the money in circulation was private bank money that the Reichsbank readily exchanged for Reichsmarks on demand. The private Reichsbank also enabled speculators short-sell the currency, which was already under severe pressure due to the transfer problem of the reparations payments pointed out by Keynes (1929).21It did so by granting lavish Reichsmark loans to speculators on demand, which they could exchange for foreign currency when forward sales of Reichsmarks matured. When Schacht was appointed, in late 1923, he stopped converting private monies to Reichsmark on demand,he stopped granting Reichsmark loans on demand, and furthermore he made the new Rentenmark non-convertible against foreign currencies. The result was that speculators were crushed and the hyperinflation was stopped. Further support for the currency came from the Dawes plan that significantly reduced unrealistically high reparations payments.This episode can therefore clearly not be blamed on excessive money printing by a government-run central bank, but rather on a combination of excessive reparations claims and of massive money creation by private speculators, aided and abetted by a private central bank.\nsources #  The Chicago Plan Revisited, Jaromir Benes and Michael Kumhof; page 16,\n"},{"id":4,"href":"/hub/posts/dao_corollary/","title":"DAO_Corollary","section":"Blog","content":"DAO Corollary, For Teams #   F.K.A. Amdahl\u0026rsquo;s Corollary\n The most efficient way to implement a piece of software is to do it all yourself.\nNo time is wasted communicating (or arguing); everything that needs to be done is done by the same person, which increases their ability to maintain the software; and the code is by default way more consistent.\nTurns out “more efficient” doesn’t mean “faster”. When there are more people working on the same problem, we can parallelized more at once.\nWhen we break work up across a team, in order to optimise for the team, we often have to put more work in, individually, to ensure that the work can be efficiently parallelized. This includes explaining concepts, team meetings, code review, pair programming, etc. But by putting that work in, we make the work more parallelized, speeding up and allowing us to make greater gains in the future.\nAn Aside: Amdahl’s Law #  Amdahl’s law can be formulated as follows:\nSlatency=1(1-p)+ps\nIn words, it predicts the maximum potential speedup (Slatency), given a proportion of the task, p, that will benefit from improved (either more or better) resources, and a parallel speedup factor, s.\nTo demonstrate, if we can speed up 10% of the task (p=0.1) by a factor of 5 (s=5), we get the following:\nSlatency=1(1-0.1)+0.15≈1.09\nThat’s about an 9% speedup. Eh, fair enough. If we can swing it, sounds good.\nHowever, if we can speed up 90% of the task (p=0.9) by a factor of 5 (s=5), we get the following:\nSlatency=1(1-0.9)+0.95≈3.58\nThat’s roughly a 250% increase! Big enough that it’s actually worth creating twice as much work; it still pays off, assuming the value of the work dwarfs the cost of the resources.\ns→∞, which means ps→0, so we can also drop the ps term if we can afford potentially infinite resources at no additional cost.\nSlatency=11-0.9=10\nIn other words, if 90% of the work can be parallelised, we can achieve a theoretical maximum speedup of 10x, or a 900% increase. This is highly unlikely, but gives us a useful upper bound to help us identify where the bottleneck lies.\nGeneralising To The Amount Of Work #  Typically, we start off with a completely serial process. In order to parallelize, we need to do more work. It doesn’t come for free.\nThis means that when computing s, the parallel speedup, we should divide it by the cost of parallelisation. For example, if the cost is 2, that means that making the work parallelisable (without actually increasing the number of resources) makes the parallel portion take twice as long as it used to. (The serial portion is unchanged.)\nSo, if we take the example from earlier, where 90% of the work is parallelisable but it costs twice as much to parallelize, we’ll get the following result:\nSlatency=1(1-0.9)+0.952≈2.18\nIt’s still about a 117% increase in output!\nHowever, if p=0.1, then there’s really very little point in adding more resources.\nSlatency=1(1-0.1)+0.152≈1.06\nAnd if the cost of parallelization is greater than the potential speedup, bad things happen:\nSlatency=1(1-0.1)+0.1520≈0.769\nAdding 4 more resources slows us down by 23%. Many of us have seen this happen in practice with poor parallelization techniques—poor usage of locks, resource contention (especially with regards to I/O), or even redundant work due to mismanaged job distribution.\nSo, What Does It All Mean? #  Amdahl’s law tells us something very insightful: when the value of your work is much greater than the cost, you should optimise for parallelism, not efficiency. The cost of a weekly two-hour team meeting is high (typically in the $1000s each time), but if it means that you can have 7 people on the team, not 3, it’s often worth it.\nDelivering faster means you can deliver more.\nDon’t stop at optimizing meetings. Pairing costs money, but it usually means way better team cohesion, which improves your ability to parallelise. Better to have 10 people working on 5 problems and doing a better job than it is to have 10 people working on 10 problems.\nThe former will lead to fewer conflicts, fewer defects and a much more motivated team. In other words (and by words I mean algebra), p and s both go up way faster than the amount of work.\nYes, meetings and other collaboration enhancers are boring. But they’re necessary. Just make sure you never lose sight of their purpose: to build a shared vision and style of work so that the team (or the organization) works as one unit. This allows us to grow the team and benefit from the increased number of people on the job.\nConversely, if all the knowledge of how the product works is in one person’s head, p≈0. While there’s no impact to efficiency this way, it limits our ability to produce, because one person can only do so much. Adding more people just makes things slower.\n"},{"id":5,"href":"/hub/posts/problems_are_hard/","title":"Problems_Are_hard","section":"Blog","content":"All Problems Are Hard #   Aren’t all problems hard?\n Say a customer reports a bug, and you’ve got to fix it. You can fix it. You know you can fix it. Maybe it’ll take an hour, maybe it’ll take a day, maybe a week. But you’ll narrow it down, figure out the smallest possible increment of work you can do to remedy the problem, and then you’ll do it. And as much as it pains me to say it, because I want to be able to check that box that said I did the thing… this is the easy part.\nOnce in a while, when we’re feeling brave, we also try to tackle the hard problem. We try to answer the questions that we usually avoid.\n Why was this bug caused in the first place? Did we know about it before the customer hit it, and if so, why didn’t we do anything about it? Was the customer upset? Did we make sure they’re happy again? Can we fix not just this bug, but all classes of this bug in the future? What are the root causes ingrained in our culture that made this bug possible? Can we change our culture to ensure this doesn’t happen again?  These are the hard problems. We can’t always solve them. Maybe they’re too entrenched, maybe you don’t have the power, maybe others don’t see the problem, maybe they’re too expensive to fix. Or maybe you just don’t have the energy to fight this battle. We can’t fight all of them, after all.\nThat way lies burnout.\n"},{"id":6,"href":"/hub/posts/uniswapv3/","title":"UniSwap V3","section":"Blog","content":"UniSwap V3 #  _For Uniswap V3, $\\left( r_1 + \\frac{C_1}{\\sqrt{\\mathcal{A}}-1} \\right) \\cdot \\left( r_2 + \\frac{C_2}{\\sqrt{\\mathcal{A}}-1} \\right) = \\frac{ \\mathcal{A} \\cdot C_1 \\cdot C_2}{(\\sqrt{\\mathcal{A}} -1)^2}$ and the formula from the white paper\n$(x+\\frac{L}{\\sqrt{pb}})(y+L\\sqrt{pa}) = L^2$\nequation (2.2) page 2 from Adams, H., Zinsmeister, N., Salem moody, M., River Keefer, uniswaporg, \u0026amp; Robinson, D. (2021). Uniswap v3 Core.\nare in fact equivalent.\nThis can be seen by equating their notation with ours in the following way:\n$L = \\frac{\\sqrt{\\mathcal{A} \\cdot C_1 \\cdot C_2}}{\\sqrt{\\mathcal{A}} -1}$\n$x = r_1$\n$y = r_1$\n$pa = \\frac{C_1 \\cdot \\mathcal{A}}{C_2}$\n$pb = \\frac{C_1}{C_2 \\cdot \\mathcal{A}}$\nIn particular, given any $L, pa, pb \u0026gt; 0$, one should be able to back out the corresponding $C_1, C_2, \\mathcal{A}$ using the equation set above and arrive at:\n$\\mathcal{A} = \\sqrt{pa \\cdot pb}$\n$C_1 = \\frac{L[(pa \\cdot pb)^\\frac{1}{4}-1]}{\\sqrt{pb}}$\n$C_2 = L[(pa \\cdot pb)^\\frac{1}{4}-1] \\sqrt{pa}$\nThus, our formula, same as the official formula, does not assume price range symmetry. There is therefore no need for $A_{lower}$ and $A_{upper}$.\n"},{"id":7,"href":"/hub/posts/math-in-solidity/","title":"Math in Solidity","section":"Blog","content":"Testing Highlighting #  contract Sqrt { function sqrt (uint x) public pure returns (uint y) { if (x \u0026gt; 0) { y = 1; uint z = x; if (z \u0026gt;= 0x100000000000000000000000000000000) { y \u0026lt;\u0026lt;= 64; z \u0026gt;\u0026gt;= 128; } if (z \u0026gt;= 0x10000000000000000) { y \u0026lt;\u0026lt;= 32; z \u0026gt;\u0026gt;= 64; } if (z \u0026gt;= 0x100000000) { y \u0026lt;\u0026lt;= 16; z \u0026gt;\u0026gt;= 32; } if (z \u0026gt;= 0x10000) { y \u0026lt;\u0026lt;= 8; z \u0026gt;\u0026gt;= 16; } if (z \u0026gt;= 0x100) { y \u0026lt;\u0026lt;= 4; z \u0026gt;\u0026gt;= 8; } if (z \u0026gt;= 0x10) { y \u0026lt;\u0026lt;= 2; z \u0026gt;\u0026gt;= 4; } if (z \u0026gt;= 0x4) { y \u0026lt;\u0026lt;= 1; } // Seven iterations should be enough for 128 bits of precision  y = (x / y + y) / 2; y = (x / y + y) / 2; y = (x / y + y) / 2; y = (x / y + y) / 2; y = (x / y + y) / 2; y = (x / y + y) / 2; y = (x / y + y) / 2; } else y = 0; } }"},{"id":8,"href":"/hub/posts/mev_and_1559/","title":"MEV and EIP 1559","section":"Blog","content":"MEV and EIP-1559 #   Kristof Gazso (Nethermind) \u0026amp; Alejo Salles (Flashbots) The incorporation of\n Kristof Gazso (Nethermind) \u0026amp; Alejo Salles (Flashbots)\nThe incorporation of EIP-1559 in the London hardfork brings a major restructuring of the Ethereum fee mechanism, aiming to allow for easier fee estimation by users and consolidate ETH as the base currency of the network by burning part of the transaction fees. This post analyzes some of the consequences of this EIP under the light of the MEV (Maximal Extractable Value) phenomenon, that is, the permissionless extraction of value by the reordering, addition, or censoring of transactions.\nUnder the new fee mechanism, instead of choosing a gas price for their transactions, users set a “priority fee” for miners to incentivize inclusion, alongside a “max fee”, stating the absolute maximum price that they are willing to pay. The protocol will now set a per-block “basefee”, computed programmatically from the amount of gas used in the block immediately before, in a negative feedback loop meant for block sizes to stabilize around a target size s0 (initially equal to the maximum current block size). Valid transactions pay a gas price equal to the basefee plus the prioirity fee (only up to the max fee–needed in case of sudden basefee increases); the prioirity fee goes to the miner, and, crucially, the basefee is burnt.\nThese changes have some obvious immediate implications for MEV-related infrastructure, like eliminating the possibility of zero-Gwei transactions, which are presently used for front-running protection in some DEXes like MistX, where miner fees are taken directly from the transferred tokens. On the other hand, we don’t expect radically new sources of MEV to arise from the novel fee mechanism. In this note, we highlight three areas where EIP-1559 might have interesting consequences for MEV: increased MEV extraction incentives for miners, the coexistence of different auctions in the Ethereum protocol and its implications for EIP design, and the role of Flashbots as a miner coordinating agent and its ethical implications.\nMiners economics and the increased incentive for MEV extraction #  Either due to proper economic incentives or because of their loyalty to the network, miners have produced blocks since the inception of Ethereum without major malicious deviations from the protocol. The Flash Boys 2.0 paper however warned of the nefarious consequences that indiscriminate MEV extraction might bring to the protocol like transaction censoring, or chain re-orgs ultimately threatening consensus stability. Notably, however, it was only recently that MEV extraction became the standard for miners when Flashbots introduced MEV-geth, a fork of the geth client that miners can run to start receiving “MEV bundles”, packaged sets of transactions that ensure a payment to miners upon inclusion. Partly due to the looming threat of EIP-1559 reducing their income, most miners were quick to adopt MEV-geth to partially palliate the shock. It is then reasonable to ask, when EIP-1559 comes into action, whether miners will devote extra efforts for further MEV extraction, in particular in ways that are harmful to the network.\nWhile we are not able to make a quantitative prediction due to the hard to quantify loyalty component, we find it useful to be able to think of MEV extraction on the same footing as other strategies miners have to compensate for lost revenue, like switching their hashrate to other GPU chains. To this effect, we provide a basic model of miner economics allowing us to estimate the required increase in Realized Extractable Value[1] to match the profit hike from switching chains post-London.\nWe start by defining a set of Proof-of-Work GPU-based chains X excluding Ethereum where miners could (we assume costlessly) point their hashrate to. We then posit that, in equilibrium, total GPU hashrate H will be distributed between Ethereum (pre- and post-London) and X in a way that maximizes profit-per-hashrate (otherwise more miners would switch until this equilibrium is attained). Finally, assuming cost-per-hashrate is constant and the same for all chains, we can derive the hashrate fraction that will remain in Ethereum after the London hard fork. This can be succintly expressed as:\nH1559HE=1+δ1+δ/γ,\nwhere H1559 is the post-London Ethereum hashrate, HE is the pre-London Ethereum hashrate, γ=R1559RE is the fraction of total revenue left for miners in Ethereum after EIP-1559, and δ=RXRE is the ratio of total X revenues to pre-London Ethereum revenues (we save the reader from the algebra, which can be checked here). The relation between these quantities can be visualized as follows:\nGas prices in Ethereum have suffered variations of two orders of magnitude, which makes it extremely hard to produce accurate estimations for the parameters of the model. We can still use data from the Flashbots dashboard and Etherchain’s data on mining reward to pick a value of γ=0.86 for the fraction of total miner revenues that will still be available in Ethereum after EIP-1559, based on recent gas prices. Finally, using CoinMetrics data to find the mining revenue in USD per day for GPU-based PoW chains other than Ethereum to find RX, we get an approximate value of δ=0.15[2]. Plugging in these value we obtain H1559HE≃98% for the fraction of hashrate staying in Ethereum after the London fork[3].\nThis constitutes an extremely simplified model of miner economics, neglecting in particular the cost of switching to other chains. It does however provide a framework to reason about MEV revenues in the broader context of miner economics. Based again on highly-varying Flashbots data, we find that, all other things being equal, the fraction of miners’ revenue coming from Realized Extracted Value will rise from 2.9% pre-London to 3.4% after the fork[4]. If, instead of switching their hashrate to other chains until attaining equilibrium miners decided to compensate for the reduced revenue by extracting more MEV, it would take them an extra 22% of extracted MEV to match the profit hike of switching lanes (which would still entail a great revenue reduction from the pre-London rates). A more detailed model including an explicit formulation of the costs of extra MEV extraction vs. chain switching is outside the scope of this note. The wildly varying nature of the values involved casts some doubt about the utility of highly elaborate models, on top of the subjective factor of miner loyalty playing a big part in miners’ course of action.\nOn this note, most mining pools have publicly opposed the EIP, feeling that they are being thrown under the bus after half a decade of maintaining Ethereum’s security. Tim Roughgarden’s analysis of EIP-1559 points out that miner loyalty might be a major reason why there haven’t been any major disruptions to Ethereum’s consensus since genesis, even if deviations from the protocol might have temporarily yielded increased profits. This might make a decrease in miner revenue that arises from the implementation of this EIP more dangerous than if it came from a drop in the price of ETH, since it comes accompanied by a loss of trust on the miners’ side. Ultimately, we will need to wait until the EIP goes live to see the extent to which miners start going to extra lengths to extract more MEV in ways detrimental to general network health, as we discuss later in the post.\nMany auctions in one #  The fee mechanism put forth in EIP-1559 was designed (and later analyzed) with an auction for transaction inclusion in mind. In fact, however, much of the activity in Ethereum is concerned with not only inclusion but also transaction ordering within a block. Most MEV extraction opportunities depend on the relative positioning of transactions, just having transactions included won’t cut it. The current first-price mechanism is limited in that users can express their wish to be included earlier than a certain target transaction, or, at most, aim at landing just after one by picking its exact gas price and hoping luck will be in their favor (backrunning). This limitation has led to systems like Flashbots, which provide a richer language for expressing preferences (users can bid for precise relative ordering of transaction sets, the “MEV bundles” mentioned above).\nAnother desirable property not provided by the current pricing mechanism is privacy. Searchers extracting MEV typically wish their maneuvers to be kept private–at least until they are inevitably made public when mined in a block–to prevent other actors from stealing their lot. Flashbots, as well as other private pool providers, offer this kind of guarantee (apart from uncle risk or miner misbehavior). Recently, several user-facing applications like MistX and 1inch have turned to Flashbots to provide users front-running protection in their trades. If an increasingly larger fraction of the network is concerned with bidding for ordering and privacy as opposed to simply inclusion, this begs the question whether EIP-1559 brings any advantage at all, or will instead be entirely superseded by Flashbots-like, more expressive mechanisms. In an extreme scenario, these different coexisting auctions could show negative interactions, where users participating in one of the auctions disrupt the other one.\nThis richer landscape where users care for ordering and privacy apart from inclusion raises concerns about the actual applicability of rigorous formal analyses like Tim Roughgarden’s (cited above). In his paper, he shows that the EIP-1559 mechanism is Myopic Miner Incentive Compatible (MMIC, meaning that miners are incentivized to act according to it in the single-block scale), Off-Chain Agreement-proof (OCA-proof, meaning that users and miners cannot outgame the system by using off-chain communication), and, apart from periods of high-demand, User Incentive Compatible (UIC, meaning that it achieves the desired UX improvement of users expressing their true preferences for inclusion without needing to speculate on other users’ actions). These conclusions rely however on the “purely inclusion” auction, and it’s not clear how they would hold out in the current, more complex setting.\nIn particular, the paper discusses an alternative proposal, the “tipless mechanism”, where there’s no miner priority fee, and which is shown to be MMIC, always UIC, and OCA-proof apart from periods of high demand. In short, it trades OCA-proofness for preserving the UX improvements throughout. The fact that, unlike what is suggested in the paper, Off-Chain Agreements are nowadays common via systems like Flashbots suggests that perhaps a tipless mechanism would have been a better choice than 1559’s basefee + priority, optimizing the standard auction for regular users who only care about inclusion and directing more sophisticated users to ordering-focused auctions.\nIt is unfortunately extremely hard to rigorously model the ordering auction (consider for instance that the allocation rule cannot be simply expressed in terms of binary–included or not–values, or that the feasibility condition needs to take into account the interaction between transactions and hence cannot be expressed as a single inequality). Instead, it might be more useful to think of the different auctions separately and analyze their potential interactions, as pointed out before. To give one example of one such interaction we can take the case of μ, the marginal cost of including a transaction. In an MEV-less world this could be thought of as a constant, and has actually been estimated. In the MEV-rich setting, however, a huge MEV reward present in the mempool might render it pointless to even run a regular auction for inclusion in the block, greatly raising greatly the value of μ, making it in turn impossible for users to converge on a reasonable value for the priority fee[5].\nTo summarize, we have several kinds of auctions coexisting in Ethereum these days:\nAuctioned good\nMechanism\nTransaction inclusion\nPGAs/EIP-1559 priority fees\nTransaction privacy\nFlashbots/Darkpools\nTransaction ordering\nFlashbots/Other relays\nAt Flashbots, we are considering making the auction mechanism more expressive to cover the use case of users that do not care about ordering but simply want front-running protection. Even the current pricing formula might be biased to favor some kinds of extractions over others (arbitrages vs. liquidations in particular). As was pointed out above, this de facto tier system might not be optimal from a design perspective, considering it arose as a collage of actors trying to solve parts of the problem individually. In traditional finance, different instruments (stocks, options, etc.) have independent markets to accommodate their varying characteristics. It is an active area of research and a long term goal for Flashbots to craft a “Turing complete” auction where users can express arbitrary preferences in an efficient way[6]. Ultimately, this calls for more discussion around the various coexisting auctions so that a more cohesive approach can be taken directly at the protocol level. This is particularly relevant in view of the upcoming changes needed for Ethereum 2.0.\nFlashbots ethics #  As stated above, Flashbots has introduced a way for searchers to express their transaction ordering preferences to miners, leading to a more efficient market all Ethereum users should ideally benefit from. In order to achieve this, Flashbots provides custom mining software (MEV-geth) to a number of miners jointly controlling the vast majority of Ethereum’s hashrate (85% at the time of writing). This amounts to Flashbots having established an effective off-chain miner coordination mechanism, which naturally invites an exploration of its potentially nefarious implications. In particular, Flashbots has created a new Schelling point where miners can passively coordinate by adopting its software and defaults–not unlike geth before it.\nWhen talking about miner coordination here we refer only to within-protocol coordination, that is, miner joint action that could be detrimental to the network but would still be admissible at the consensus level (i.e. no double spending, or otherwise disregarding the Ethereum protocol). True 51% attacks would severely damage the network, which is generally against the interest of miners. Instead, we focus on the more “benign” case where miners coordinate for their own benefit against other actors’ interests, while abiding by the protocol rules. Short of analyzing how a focal point for miner coordination can affect the network (since, again, this was already available via geth upgrades), we ask the more specific question of whether the introduction of EIP-1559 increases the span of potentially nefarious consequences of Flashbots as a miner coordination mechanism.\nWe have already covered one way in which EIP-1559 changes the landscape, independently of Flashbots, by increasing the relative weight of MEV rewards within the total share of miner revenue. One way Flashbots affects this is via the share of block space dedicated to bundles as opposed to regular transactions, or the relative weight of the different auctions described in the previous section. MEV-geth has a setting for the maximum number of bundles considered for inclusion in a block, spawning a worker to build one template block for each number of bundles from 0 to this configurable maximum, and all template blocks are later compared for profitability. The larger this setting, the more profitable blocks might become, and the less block space left for standard user transactions. EIP-1559 incentivizing miners to go after more MEV might result in them being more accepting of higher and higher values, ultimately leading to MEV-only blocks. This scenario is however unlikely to play out, as there is an opportunity cost of including bundles by leaving transactions with high priority fees out. Altogether, we don’t believe that EIP-1559 increases the risk of known nefarious behaviors facilitated by Flashbots.\nEIP-1559, however, introduces a new undesirable behavior into consideration: miners coordinating to mine less-than-target sized blocks to drive the baseefee down to 0, which would effectively do away with the UX improvements that the EIP seeks to provide. Again, Flashbots is in a privileged position to enable this by providing an update that implements this behavior. More concretely, instead of including all transactions with gas limit above the basefee with greater than zero priority fee, the software could target a block size of s0(1−ϵ), where s0 is the target block size and ϵ∈[0,1] is a parameter deliberately set to drive and keep the basefee down, dependent on the hashrate fraction running MEV-geth.\nWe leverage the Ethereum Foundation Robust Incentives Group’s framework to explore the viability of this attack (a notebook with the code for these results can be found here). Taking a simple oscillatory demand scenario and setting pFB=0.85 for the probability of the miner running MEV-geth, we simulate the system dynamics for different values of ϵ, illustrating how for a large enough value the basefee is driven to 0:\nAs in the previous case, there is an immediate economic downside for miners to go along with this strategy since they would be leaving money on the table with each block.\nThe next natural question to ask is how long would miners need to work at a loss until the drop in basefee finally compensates so that them colluding proves profitable. In simulations, we found this to be extremely sensitive to the nature of the demand, depending in particular on whether users would continue offering bids close to their true value or instead lower their bids by adapting to the new, artificially low basefee. While we cannot provide a meaningful answer until we see the EIP-1559 mechanism in action, we raise attention to the issue, while noting that even if such a collusion proves profitable after a short period of time, it could be broken at any point by a selfish miner including a larger number of transactions, and is thus not very likely to occur in practice.\nConclusions #  We found no critical way in which EIP-1559 interacts with MEV extraction. However, we identified several areas where new dynamics might take place, in particular around miners’ incentives to extract more MEV or defeat the new fee mechanism by passively colluding around a potential nefarious Flashbots’ software update. Needless to say, it is not in the interest of Flashbots to provide such an update, but having 85% of Ethereum’s hashrate running MEV-geth requires us to be thoughtful about the implications. Finally, we suggest that despite EIP-1559 being designed with a transaction inclusion auction in mind, many different auctions are nowadays taking place in Ethereum. It is critical to acknowledge this and design our systems to accommodate for these different auctions if we want to live up to Ethereum’s open standards.\nWe would like to thank Barnabé Monnot and the Robust Incentives Group at the Ethereum Foundation, Leo Zhang, Tim Beiko, and Tina Zhen and the entire Flashbots team for conversations leading to the ideas presented here.\n   Realized Extractable Value is defined in our previous post on quantifying MEV extraction. ↩︎\n  Dogecoin, Ethereum Classic, ZCash, Dash, Monero, Bitcoin Gold, and Verge were included for the calculations of chain X revenues. ↩︎\n  At the time of writing, the values were calculated as follows:\nRE=rBLOCK REWARDS+rTX FEES+rMEV=36,390,017USD and\nR1559=rBLOCK REWARDS+rMEV=31,327,517USD, where:\nrBLOCK REWARDS=13450Ξ×PETH,\nrTX FEES=2250Ξ×PETH,\nrMEV=1,065,017USD,\nand PETH=2250USD ↩︎\n  All else being equal, the basefee burning does not decrease Realized Extracted Value if we assume that the amount burned by the bundles will be equivalent to the one burned by the tail transactions that get pushed out of the block. ↩︎\n  Though see this note for an approach to bounding the uncle risk in an MEV-rich context. ↩︎\n  Auction design is a rich field of study with many results focusing on computability of preferences/allocations, see Tim Roughgarden’s lecture notes for an introduction. ↩︎\n  "},{"id":9,"href":"/hub/posts/evm-lowlevel/","title":"(EVM) Primer","section":"Blog","content":"Data Representation in Solidity #  For writers of line debuggers and other debugging-related utilities.\n #  \nPurpose of this document #  The point of this document is to explain representation of data in Solidity for the purposes of locating and decoding it; more specifically, for writing a line debugger that does such. As such, other information about the type system or data layout that aren\u0026rsquo;t necessary for that may be skipped; and where location is not entirely predictable but may be determined by other systems of the debugger, we may rely on that. See the Solidity documentation for things not covered here, particularly the section on types and the ABI specification; and perhaps also see the Ethereum yellow paper.\nThis document is also primarily only concerned with variables that a user might define, not special language-defined variables which will typically not be stored in any of these locations, and so for the most part we will not discuss these, although we will make an exception for the special variables msg.data and msg.sig.\nFinally this document is only concerned with variables as they exist in the Solidity language, and not in the underlying implementation; thus we will say things like \u0026ldquo;calldata cannot directly contain value types\u0026rdquo;, simply because Solidity will not allow one to declare a calldata variable of value type (the original value in calldata will always be copied onto the stack before use). Obviously the value still exists in calldata, but since no variable points there, it\u0026rsquo;s not our concern.\nNote: This document pertains to Solidity v0.8.9, current as of this writing.\n\nContents #   Purpose of this document Contents Locations: Basics Types Overview  Terminology Types and locations  Table of types and locations   Overview of the types: Direct types  Basics of direct types: Packing and padding Table of direct types Representations of direct types Presently unstoreable functions   Overview of the types: Multivalue types Overview of the types: Lookup types  Table of lookup types   Overview of the types: Pointer types  Table of pointer types     Locations in Detail  The stack in detail  The stack: Direct types and pointer types The stack: Data layout   Code in detail  Code: direct types Code: data layout   Memory in detail  Memory: Direct types and pointer types Layout of immutables in memory Memory: Multivalue types Memory: Lookup types Pointers to memory   Calldata in detail  Slots in calldata and the offset Calldata: Direct types and pointer types Calldata: Multivalue and lookup types (reference types)  The special variable msg.data   Pointers to calldata Pointers to calldata from calldata Pointers to calldata from the stack   Storage in detail  Storage: Data layout Storage: Direct types Storage: Multivalue types Storage: Lookup types Pointers to storage      \nLocations: Basics #  [ ∧ Back to contents ]\nThe EVM has a number of locations where data can be stored. We will be concerned with five of them: The stack, storage, memory, calldata, and code. (We will ignore returndata. There are also some other \u0026ldquo;special locations\u0026rdquo; that I will mention briefly in the calldata section but will mostly ignore.)\nThe stack and storage are made of words (\u0026ldquo;slots\u0026rdquo;), while memory, calldata, and code are made of bytes; however, we will basically ignore this distinction. We will, for the stack and storage, conventionally consider the large end of each word to be the earlier (left) end; and, for the other locations, conventionally consider the location as divided up into words (\u0026ldquo;slots\u0026rdquo;) of 32 bytes, with the earlier end of each word being the large end. Or, in other words, everything is big-endian (or construed as big-endian) unless stated otherwise. With this convention, we can ignore the distinction between the slot-based locations and the byte-based locations. (My apologies in advance for the abuse of terminology that results from this, but I think using this convention here saves more trouble than it causes.)\n(For calldata, we will actually use a slightly different convention, as detailed later, but you can ignore that for now. We will also occasionally use a different convention in memory, as also detailed later, but you can again ignore that for now. Also, we will ignore the notion of \u0026ldquo;slots\u0026rdquo; in the case of code.)\nMemory (with one exception to be described later) and calldata will always be accessed through pointers to such; as such, we will only discuss concrete data layout for storage, the stack, and code, as those are the only locations we\u0026rsquo;ll access without a pointer (but for the stack we\u0026rsquo;ll mostly rely on the debugger having other ways of determining location, and for code we\u0026rsquo;ll rely on other compiler output).\n\nTypes Overview #  [ ∧ Back to contents ]\n Terminology Types and locations  Table of types and locations   Overview of the types: Direct types  Basics of direct types: Packing and padding Table of direct types Representations of direct types Presently unstoreable functions   Overview of the types: Multivalue types Overview of the types: Lookup types  Table of lookup types   Overview of the types: Pointer types  Table of pointer types    \nTerminology #  [ ∧ Back to Types Overview ]\nThere are a number of ways of dividing up the types into classes. The system I\u0026rsquo;ll use here is my own, based on what I think is useful here.\nA variable of direct type can, for our purposes, be considered as a value by itself.\nA variable of multivalue type holds a fixed number of other element variables, stored consecutively.\nA variable of lookup type holds a number of other element variables not fixed in advance.\nA variable of pointer type holds a reference to another multivalue or lookup variable to be found elsewhere. They never point to variables of direct type or other variables of pointer type. (Note that pointers are not, in Solidity, an actual type separate from that of what they point to, but they\u0026rsquo;re useful to consider as a separate type here.)\nThis will be our fourfold division of types. Some other type terminology, as defined by the language, is useful:\nThe term reference types refers collectively to multivalue and lookup types.\nA static type is either\n A direct type, or A multivalue type, all of whose element variables are also of static type.  (Remark: In pre-0.5.0 versions of Solidity, when static-length of arrays of length 0 were allowed, these were automatically static regardless of the base type, since, after all, there are no element variables.)\nA dynamic type is any type that is not static. (Pointers don\u0026rsquo;t fit into this dichotomy, not being an actual Solidity type.)\nWe\u0026rsquo;ll also use the term key types to denote types that can be used as mapping keys. See the section on lookup types for more on these.\nFinally, to avoid confusion with other meanings of the word \u0026ldquo;value\u0026rdquo;, I\u0026rsquo;m going to speak of \u0026ldquo;keys and elements\u0026rdquo; rather than \u0026ldquo;keys and values\u0026rdquo;; I\u0026rsquo;m going to consistently speak of \u0026ldquo;elements\u0026rdquo; rather than \u0026ldquo;values\u0026rdquo; or \u0026ldquo;children\u0026rdquo; or \u0026ldquo;members\u0026rdquo;.\n\nTypes and locations #  [ ∧ Back to Types Overview ]\nWhat types can go in what locations?\nThe stack can hold only direct types and pointer types.\nDirectly pointed-to variables living in memory or calldata can only be of reference type, although their elements, also living in memory or calldata, can of course also be of direct or pointer type.\nSome types are not allowed in calldata, especially if ABIEncoderV2 is not being used; but we will assume it is. Note, though, that circular types are never allowed in calldata.\nOnly direct types may go in code as immutables; moreover, variables of type function external cannot presently go in code this way. (Nor can they go in memory as immutables, when memory is being used to store immutables.)\nIn addition, the locations memory and calldata may not hold mappings, which may go only in storage. (However, structs that contain mappings, or that contain (possibly multidimensional) arrays of mappings, were allowed in memory prior to Solidity 0.7.0, though such mappings or arrrays would be omitted from the struct; see the section on memory for more detail.)\nStorage does not hold pointer types as there is never any reason for it to do so.\nWhile this is not a type-level concern, it is likely worth noting here that memory (and no other location) can contain circular structs. Storage can also contain structs of circular type, but not actual circular structs.\nNote that reference types, in Solidity, include the location as part of the type (with the exception of mappings as it would be unnecessary there); however we will ignore this from here on out, since if we are talking about a particular location then obviously we are talking only about types that go in that location.\nThe rest of this section will give a brief overview of the various types. However, one should see the appropriate location sections for more information. Still, here is a summary table one may use (this also covers some things not mentioned above):\n\nTable of types and locations #     Location Direct types Multivalue types Lookup types Mappings and arrays of such in structs are\u0026hellip; Pointer types     Stack Yes No (only as pointers) No (only as pointers) N/A To storage, memory, or calldata   Storage Yes Yes Yes Legal No   Memory Only as elements of other types or as immutables Yes Yes, excluding mappings Illegal (omitted prior to 0.7.0) To memory (only as elements of other types)   Calldata Only as elements of other types, with restrictions Yes, excluding circular struct types Yes, excluding mappings Illegal To calldata (only as elements of other types)   Code Yes, with restrictions No No N/A No    Note that with the exception of the special case of mappings (or possibly multidimensional arrays of such) in structs, it is otherwise true that if the type of some element of some given type is illegal in that location, then so is the type as a whole. Also, immutables in memory have the same restrictions that they do in code.\n\nOverview of the types: Direct types #  [ ∧ Back to Types Overview ]\n\nBasics of direct types: Packing and padding #  With regard to direct types, storage is a packed location \u0026ndash; multiple variables of direct type may share a storage slot, within which each variable only takes up as much space as it need to; see the table below for information on sizes. (Note that variables of direct type may not cross word boundaries.)\nThe stack, memory, calldata, and code, however, are padded locations \u0026ndash; each variable of direct type always takes up a full slot. (There are two exceptions to this \u0026ndash; the individual bytes in a bytes or string are packed rather than padded; and external functions take up two slots on the stack. Both these will be described in more detail later (1, 2).) The exact method of padding varies by type, as detailed in the table below. Note that immutables have slightly unusual padding, whether stored in code or memory, as will be detailed later (1, 2).\n(Again, note that for calldata we are using a somewhat unusual notion of slot; see the calldata section for more information.)\n\nTable of direct types #  Here is a table of all the (general classes of) direct types and their key properties. Some of this information may not yet make sense if you have only read up to this point. See the next section for more detail on how these types are actually represented.\n   Type Size in storage (bytes) Padding in padded locations Default value Is key type? Allowed in calldata? Allowed as immutable? Can back a UDVT?     bool 1 Zero padded, left false Yes Yes Yes Yes   uintN N/8 Zero-padded, left* 0 Yes Yes Yes Yes   intN N/8 Sign-padded, left* 0 Yes Yes Yes Yes   address [payable] 20 Zero-padded, left* Zero address (not valid!) Yes Yes Yes Yes   contract types 20 Zero-padded, left* Zero address (not valid!) Yes Yes Yes Yes   bytesN N Zero-padded, right* All zeroes Yes Yes Yes No   enum types As many as needed to hold all possibilities Zero-padded, left Whichever possibility is represented by 0 Yes Yes Yes Yes   function internal 8 Zero-padded, left Depends on location, but always invalid No No Yes No   function external 24 Zero-padded, right, except on stack Zero address, zero selector (not valid!) No Yes No No   ufixedMxN M/8 Zero-padded, left* 0 Yes Yes Yes Yes   fixedMxN M/8 Sign-padded, left* 0 Yes Yes Yes Yes   User-defined value types Same as underlying type (except in 0.8.8) Same as underlying type* Same as underlying type Yes Yes Yes No    Some remarks:\n As the table states, external functions act a bit oddly on the stack; see the section on the stack for details. In Solidity 0.8.8, there is a bug that caused user-defined value types to always take up one full word in storage, regardless of the size of the underlying type; they would be padded as if they were being stored in a padded location. Prior to Solidity 0.8.9, padding worked a bit differently in code; in code, all types were zero-padded, even if they would ordinarily be sign-padded. This did not affect which side they are padded on. Prior to Solidity 0.8.9, padding also worked a bit differently for immutables stored in memory during contract construction. In this context, all types used to be zero-padded on the right, regardless of their usual padding. Some types are marked with an asterisk regarding their padding. These types may have incorrect padding while on the stack due to operations that overflow. Solidity will always restore the correct padding when it is necessary to do so; however, it will not do this until it is necessary to do so. So, be aware that on the stack these types may be padded incorrectly. The ufixedMxN and fixedMxN types are not implemented yet. Their listed properies are largely inferred based on what we can expect. Some direct types have aliases; these have not been listed in the above table. uint and int are aliases for uint256 and int256; ufixed and fixed for ufixed128x18 and fixed128x18; and byte for bytes1. Each direct type\u0026rsquo;s default value is simply whatever value is represented by a string of all zero bytes, with the one exception of internal functions in locations other than storage. See below for more on this. The N in uintN and intN must be a multiple of 8, from 8 to 256. The M in ufixedMxN and fixedMxN must be a multiple of 8, from 8 to 256, while N must be from 0 to 80. The N in bytesN must be from 1 to 32. Function types are, of course, more complex than just their division into internal and external; they also have input parameter types, output parameter types, and mutability modifiers (pure, view, payable). However, these will not concern us here, and we will ignore them.  \nRepresentations of direct types #  uintN is an N-bit binary number (big-endian). The signed variant intN uses 2\u0026rsquo;s-complement.\nbytesN is simply a string of N bytes.\nBooleans are represented by 0 for false and 1 for true; they act like uint8, just restricted to 0 and 1.\nAddresses just act like uint160. Contracts are represented by their underying addresses.\nEnums are represented by integers; the possibility listed first by 0, the next by 1, and so forth. An enum type just acts like uintN, where N is the smallest legal value large enough to accomodate all the possibilities.\nInternal functions may be represented in one of two ways. The bottom 4 bytes represented by the code address (in bytes from the beginning of code) of the beginning of said function (specifically, the JUMPDEST instruction that begins it). If the value was set outside a constructor, the 4 bytes above that will be 0. However, inside a constructor, the 4 bytes above that will instead be the code address of the function inside the constructor code rather than the deployed code.\nFor internal functions, default values are also worth discussing, as in non-storage locations, they have a nonzero default value. In contracts for which Solidity deems it necessary, there will be a special designated invalid function. In Solidity 0.8.0 and later, this function throws a Panic(0x51); in earlier versions of Solidity, it uses the INVALID opcode, reverting the transaction and consuming all available gas. In versions of Solidity prior to 0.8.0, it has the bytecode 0x5bfe; in Solidity 0.8.0 to 0.8.4, it has the bytecode\n0x5b7f000000000000000000000000000000000000000000000000000000004e487b71600052605160045260246000fd and in Solidity 0.8.5 and later, it is a function that just jumps directly to a function with the latter bytecode (this jumped-to function may also be identified by the fact that it is a generated Yul function with name panic_error_0x51). As mentioned, in all cases, such a function is only included if Solidity deems it necessary. The default value for an internal function, then, outside of storage, is to point to this designated invalid function. In all other respects, these default values are encoded as above.\nRemark: Prior to Solidity 0.5.8 (or Solidity 0.4.26, in the 0.4.x line) there was a bug causing the default value for internal functions to be incorrectly encoded when it was set in a constructor. It would have 0 for the upper 4 bytes, and would have as the lower 4 bytes what the upper 4 bytes should have been.\nExternal functions are represented by a 20-byte address and a 4-byte selector; in locations other than the stack, this consists of first the 20-byte address and then the 4-byte selector. On the stack, however, it is more complicated. See the section on the stack for details.\nufixedMxN and fixedMxN are interpreted as follows: If interpreting as a (M-bit, big-endian) binary number (unsigned or signed as appropriate) would yield k, the result is interpreted as the rational number k/10**N.\nUser-defined value types are always backed by another type (see the table for which types are allowed in this context). Their representation is the same as that of the underlying type.\n\nPresently unstoreable functions #  Some legal values of function type presently have no representation and so cannot be stored in a variable. These are:\n External functions with a specified amount of gas or value attached (even if that amount is zero). External functions of libraries \u0026ndash; including library functions declared public when not in that library \u0026ndash; because there is presently no way to represent that they should be called with DELEGATECALL, and because they may accept non-ABI types and thus have no signature according to the ABI specification. This similarly includes functions created by using ... for ... directives. Special functions defined by the language. This means globally available functions; functions which are members of arrays; functions which are members of addresses; and functions which are members of external functions.  So, the question of how these are presently represented when stored, is that they are not. (There are other presently unstoreable functions, too, but since their unstorability is due to other issues, we will not discuss them here.)\n\nOverview of the types: Multivalue types #  [ ∧ Back to Types Overview ]\nThe multivalue types are type[n] (here n must be positive), which has n elements of type type; and the various user-defined struct types, whose multiple element variables (of which there must be at least one) are as specified in the appropriate struct definition, and occur in the order specified there.\nRemark: Prior to Solidity 0.5.0, it was legal to have type[0] or empty structs.\nNote that it is legal to include a mapping type, or a (possibly multidimensional) array of mappings, as an element of a struct type; prior to Solidity 0.7.0, this did not preclude the struct type from being used in memory (even though, as per the following section, mappings cannot appear in memory), but rather, the mapping (or array) would be simply omitted in memory. See the memory section for more details. Such a struct has always been barred from appearing in calldata, however.\nAlso note that circular struct types are allowed, so long as the circularity is mediated by a lookup type. That is to say, if a struct type T0 has a element type T1 which has a element type \u0026hellip; which has a element type Tn with Tn equal to T0, this is legal only if at least one of the types Ti is a lookup type. However, such types are allowed only in storage and memory, not calldata.\nThe default value for a multivalue type consists of assigning the default value to each of its element variables.\n(There\u0026rsquo;s no table for this section as there would be little point.)\n\nOverview of the types: Lookup types #  [ ∧ Back to Types Overview ]\nThe lookup types are type[]; mapping(keyType =\u0026gt; elementType); bytes; and string.\nDynamic arrays, type[], have an indefinite number of elements of type type. Mappings, mapping(keyType =\u0026gt; elementType, have an indefinite number of elements of type elementType. Bytestrings, bytes, have an indefinite number of elements of type byte.\nThe type string is something of a special case; strings are UTF-8 encoded to form a string of bytes, and then that string of bytes is stored exactly as if it were a bytes. For this reason, we will basically ignore the type string from here on out; it basically acts exactly like bytes, except that one cannot meaningfully speak of its elements.\nAs mentioned above, mappings can go only in storage (but see previous section about mappings in structs). Only certain types are allowed as key types for mappings; these can roughly be described as the \u0026ldquo;value types\u0026rdquo; together with string and bytes, but one should see the appropriate tables to see which direct or lookup types are key types. Observe that key types may all be meaningfully converted to a string of bytes.\nThe default value for a lookup type is for it to be empty. For the particular case of a type[] in memory, the default value once it has been initialized to a particular size is for all its elements to have their default value.\nThe information above is also summarized in the following table.\n\nTable of lookup types #     Type Element type Restricted to storage? Is key type?     type[] type No No   mapping(keyType =\u0026gt; elementType) elementType Yes No   bytes byte (bytes1) No Yes   string N/A, but underlying bytes has byte elements No Yes    Note that mappings have other special features \u0026ndash; e.g., they cannot be copied or deleted \u0026ndash; but we will not go into that here.\n\nOverview of the types: Pointer types #  [ ∧ Back to Types Overview ]\nPointers usually take up a single word, although some take up two words. See the appropriate location section for information on pointers to that location (1, 2, 3), but you may find a summarizing table below.\nAgain, remember that pointers are not, in Solidity, an actual type separate from that of what they point to, but we\u0026rsquo;re considering them here separately all the same.\nPointers always either point from the stack to somewhere else, or from one location to somewhere else in that same location. Pointers never go between different non-stack locations.\nThe default value for a memory pointer to a variable of lookup type is 0x60, the null pointer; see the section on memory pointers for more information. Attempting to delete a memory pointer to a variable of multivalue type instead allocates a new instance of that type, of its default value, and sets the pointer to point at this, so memory pointers to variables of multivalue type have no fixed default value.\nThe default value for a storage pointer is a pointer to the 0 slot \u0026ndash; beware, making use of such a pointer can lead to nonsense! Don\u0026rsquo;t do this! (Note that while it is legal to leave a storage pointer uninitialized, it is not legal to delete one.)\nCalldata pointers don\u0026rsquo;t have a default value; they\u0026rsquo;re never uninitialized and it\u0026rsquo;s illegal to delete them.\n\nTable of pointer types #     Type Absolute or relative? Measured in\u0026hellip; Has second word for length? Default value     Pointer to storage Absolute Words No 0 (may be garbage, don\u0026rsquo;t use!)   Pointer to memory Absolute Bytes No 0x60 for lookup types; no fixed default for multivalue types   Pointer to calldata from calldata Relative (in an unusual way) Bytes No N/A   Pointer to calldata multivalue type from the stack Absolute Bytes No Equal to the length of calldata   Pointer to calldata lookup type from the stack Absolute (with an offset) Bytes Yes Equal to the length of calldata; length word equal to zero    \nLocations in Detail #  [ ∧ Back to contents ]\n The stack in detail  The stack: Direct types and pointer types The stack: Data layout   Code in detail  Code: direct types Code: data layout   Memory in detail  Memory: Direct types and pointer types Layout of immutables in memory Memory: Multivalue types Memory: Lookup types Pointers to memory   Calldata in detail  Slots in calldata and the offset Calldata: Direct types and pointer types Calldata: Multivalue and lookup types (reference types)  The special variable msg.data   Pointers to calldata Pointers to calldata from calldata Pointers to calldata from the stack   Storage in detail  Storage: Data layout Storage: Direct types Storage: Multivalue types Storage: Lookup types Pointers to storage    \nThe stack in detail #  [ ∧ Back to Locations in Detail ]\nThe stack, as mentioned above, can hold only direct types and pointer types. It\u0026rsquo;s also the one location other than storage that we will access directly rather than through storage, so we\u0026rsquo;ll take some time to discuss data layout on the stack.\n\nThe stack: Direct types and pointer types #  The stack is, as mentioned above, is a padded location, so all direct types are padded to a full word in the manner described in the direct types table.\nThere are two special cases that must be noted here, that each take up two words instead of one. The first special case is that of external functions. An external function is represented by a 20-byte address and a 4-byte selector; these are stored in two separate words, with the address in the bottom word and the selector in the top word. Both these are zero-padded on the left, not the right like in the other padded locations.\nThe second two-word special case is that of pointers to calldata lookup types; see the section on pointers to calldata from the stack for details.\n\nThe stack: Data layout #  Stack variables are local variables, so naturally things will change as the contract executes. But, we can still describe how things are at any given time. Note that if you are actually writing a debugger, you may want to rely on other systems to determine data layout on the stack.\nThe stack is of course not used only for storing local variables, but also as a working space. And of course it also holds return addresses. The stack is divided into stackframes; each stackframe begins with the return address. (There is no frame pointer, for those used to such a thing; just a return address.) The exceptions are constructors and fallback/receive functions, which do not include a return address. In addition, if the initial function call (i.e. stackframe) of the EVM stackframe (i.e. message call or creation call) is not a constructor, and the contract has external functions other than the constructor, fallback, and receive functions, then the function selector will be stored on the stack below the first stackframe. (Additionally, in Solidity 0.4.20 and later, an extra zero word will appear below that on the stack if you\u0026rsquo;re within a library call, unless the function called is pure or view and does not include any storage pointers in its input or output parameters.)\nNote that function modifiers and base constructor invocations (whether placed on the constructor or on the contract) do not create new stackframes; these are part of the same stackframe as the function that invoked them.\nWithin each stackframe, all variables are always stored below the workspace. So while the workspace may be unpredictable, we can ignore it for the purposes of data layout within a given stackframe. (Of course, the workspace in one stackframe does come between that stackframe\u0026rsquo;s variables and the start of the next stackframe.)\nRestricting our attention to the variables, then, the stack acts, as expected, as a stack; variables are pushed onto it when needed, and are popped off of it when no longer needed. These pushes and pops are arranged in a way that is compatible with the stack structure; i.e., they are in fact pushes and pops.\nThe parameters of the function being called, including output parameters, are pushed onto the stack when the function is called and the stackframe is entered, and are not popped until the function, including all modifiers, exits. It\u0026rsquo;s necessary here to specify the order they go onto the stack. First come the input parameters, in the order they were given, followed by the output parameters, in the order they were given. Anonymous output parameters are treated the same as named output parameters for these purposes. Similarly, parameters for fallback functions are not treated specially here, but work like any other parameters.\nRemark: Yul functions work slightly differently here, in that output parameters are pushed onto the stack in the reverse of the order they were given.\nOrdinary local variables, as declared in a function or modifier, are pushed onto the stack at their declaration and are popped when their containing block exits (for variables declared in the initializer of a for loop, the containing block is considered to be the for loop). If multiple variables are declared within a single statement, they go on the stack in the order they were declared within that statement.\nParameters to a modifier are pushed onto the stack when that modifier begins and are popped when that modifier exits. Again, they go in the stack in the order they were given. Note that (like other local variables declared in modifiers) these variables are still on the stack while the placeholder statement _; is running, even if they are inaccessible. Remember that modifiers are run in order from left to right, and that they may be applied to constructors, fallback functions, and receive functions.\nThis leaves the case of parameters to base constructor invocations (whether on the constructor or on the contract). When a constructor is called, not only are its parameters pushed onto the stack, but so are all the parameters to all of its base constructors \u0026ndash; not just the direct parents, but for all ancestors. They go on in order from most derived to most base, as determined by the usual C3 order (discussed more in the section on storage layout below). Note that if the base constructors are listed on the constructor declaration, the order has no effect; only the order that the base classes are listed on the class declaration matters here. Within each base constructor\u0026rsquo;s parameter region, the parameters are pushed on in order from left to right. Constructors then execute in order from most base to most derived (again, note that the order they\u0026rsquo;re listed on the constructor declaration has no effect); when a constructor exits, its parameters are popped from the stack. Modifiers on a constructor or base constructor are handled when that constructor or base constructor runs.\nParamters to a modifier on a fallback or receive function work like parameters to a modifier on any other function. Note that parameters to a modifier on a constructor only go onto the stack when that particular constructor is about to run (i.e., all base constructors that run before it have exited).\n\nCode in detail #  [ ∧ Back to Locations in Detail ]\nOnce a contract has been deployed, its immutable state variables are stored in its code.\n\nCode: direct types #  Only direct types may go in code as immutables. In addition, function external variables are currently barred from being used as immutables.\nNote that while code is a padded location, prior to Solidity 0.8.9, its padding worked a bit unusually. In code, all types would be zero-padded, even if ordinarily they would be sign-padded. Note that this did not alter whether they are padded on the right or on the left. Since Solidity 0.8.9, however, types in code are just padded normally.\n\nCode: data layout #  Where in the code immutables may be found is basically unpredictable in advance. However, you may use the Solidity compiler\u0026rsquo;s immutableReferences output to determine this information. Note that immutables that are never actually read from will not appear here \u0026ndash; as they won\u0026rsquo;t actually appear anywhere in the code, either! Immutables are simply inlined into the code wherever they\u0026rsquo;re read from, so if they\u0026rsquo;re never read from, their value isn\u0026rsquo;t actually stored anywhere.\nNote that code has no notion of \u0026ldquo;slots\u0026rdquo;; the variables are simply placed wherever the compiler places them, among the code.\n\nMemory in detail #  [ ∧ Back to Locations in Detail ]\nMemory is used in two different ways. Its ordinary use is to hold variables declared as living in memory. Its secondary use, however, is to hold immutables during contract construction.\nWe won\u0026rsquo;t discuss layout in memory in the first context, since, as mentioned, we only access it via pointers. However, we will discuss layout in memory for the case of immutables in memory.\nRemark: Although memory objects ordinarily start on a word, there is a bug in versions 0.5.3, 0.5.5, and 0.5.6 of Solidity specifically that can occasionally cause them to start in the middle of a word. In this case, for the purposes of decoding that object, you should consider slots to begin at the beginning of that object. (Of course, once you follow a pointer, you\u0026rsquo;ll have to have your slots based on that pointer. Again, since we only access memory through pointers, this is mostly not a concern, and it only happens at all in those specific versions of Solidity.)\n\nMemory: Direct types and pointer types #  Memory is a padded location, so direct types are padded as described in their table. Pointers, as mentioned above, always take up a full word.\nNote that prior to Solidity 0.8.9, immutables stored in memory had unusual padding; they were always zero-padded on the right, regardless of their usual padding. Again, note that this only applied to immutables stored directly in memory during contract construct, and not to direct types appearing as elements of another type in memory in memory\u0026rsquo;s normal use. Since Solidity 0.8.9, all direct types stored in memory, including immutables, have had normal padding.\n\nLayout of immutables in memory #  Immutable state variables are stored in memory during contract construction. (Or at least, for most of it; towards the end of contract construction memory will be overwritten by the code of the contract being constructed.)\nImmutable state variables are stored one after the other starting at memory address 0x80 (skipping the first four words of memory as Solidity reserves these for internal purposes). Memory being a padded location, each takes up one word (although note that as per the previous subsection, the padding on immutables was unusual prior to Solidity 0.8.9). This just leaves the question of the order that they are stored in.\nFor the simple case of a contract without inheritance, the immutable state variables are stored in the order that they are declared. In the case of inheritance, the variables of the base class go before those of the derived class. In cases of multiple inheritance, Solidity uses the C3 linearization to order classes from \u0026ldquo;most base\u0026rdquo; to \u0026ldquo;most derived\u0026rdquo;, and then, as mentioned above, lays out variables starting with the most base and ending with the most derived. (Remember that, when listing parent classes, Solidity considers parents listed first to be \u0026ldquo;more base\u0026rdquo;; as the Solidity docs note, this is the reverse order from, say, Python.)\n\nMemory: Multivalue types #  A multivalue type in memory is simply represented by concatenating together the representation of its elements; with the exceptions that elements of reference type (both multivalue and lookup types), other than mappings, are represented as pointers. (Also, prior to Solidity 0.7.0, elements of mapping type, as well as (possibly multidimensional) arrays of such, were allowed in memory structs and were simply omitted, as mappings cannot appear in memory.) As such, each element (that isn\u0026rsquo;t omitted) takes up exactly one word (because direct types are padded and all reference types are stored as pointers). Elements of structs go in the order they\u0026rsquo;re specified in.\n(Note that prior to Solidity 0.7.0 it was possible to have in memory a struct that contains only mappings, and prior to 0.5.0, it was possible to have a struct that was empty entirely, or a statically-sized array of length 0. Such a struct or array doesn\u0026rsquo;t really have a representation in memory, since in memory it has zero length. Of course, since we only access memory through pointers, if we are given a pointer to such a struct or array, we need not decode anything, as all of the struct\u0026rsquo;s elements have been omitted. The actual location pointed to may contain junk and should be ignored.)\nNote that it is possible to have circular structs \u0026ndash; not just circular struct types, but actual circular structs \u0026ndash; in memory. This is not possible in any other location.\n\nMemory: Lookup types #  There are two lookup types that can go in memory: type[] and bytes (there is also string, but we will not treat that separately from bytes).\nA dynamic array of type type[] is represented by a slot containing the length of the array (call it n), followed immediately by the array itself, represented just as if it were an array of type type[n]; see the section above.\nA bytes is represented by a slot containing the length of the bytestring, followed by a sequence of slots containing the bytestring; the bytes in the string are not individually padded, but rather are simply stored in sequence. Since the last slot may not contain a full 32 bytes, it is zero-padded on the right.\nRemark: In a few specific versions of Solidity, there is a bug that can cause particular bytes and strings to lack the padding on the end, resulting in the alignment bug mentioned above.\n\nPointers to memory #  Pointers to memory are absolute and given in bytes. Since memory is padded, all pointers will point to the start of a word and thus be a multiple of 0x20. (With the exception, mentioned above, of some pointers in some specific versions of Solidity.)\nThe pointer 0x60 is something of a null pointer; it points to a reserved slot which is always zero. By the previous section, this slot can therefore represent any empty variable of lookup type in memory, and in fact it\u0026rsquo;s used as a default value for memory pointers of lookup type.\n\nCalldata in detail #  [ ∧ Back to Locations in Detail ]\nCalldata is largely the same as memory; so rather than describing calldata from scratch, we will simply describe how it differs from memory.\nImportantly, we will use a different convention when talking about \u0026ldquo;slots\u0026rdquo; in calldata; see the following subsection. (Although it\u0026rsquo;s not that important, since, like with memory, we only access calldata through pointers. You just don\u0026rsquo;t want to find yourself surprised by it.)\n\nSlots in calldata and the offset #  The first four bytes of calldata are the function selector, and are not followed by any padding. As such, in calldata, we consider words and slots to begin not on the usual word boundaries (multiples of 0x20) but rather to begin offset by 4-bytes; \u0026ldquo;slots\u0026rdquo; in calldata will begin at bytes whose address is congruent to 0x4 modulo 0x20. (Since calldata is byte-based rather than word-based, this offset is not disastrous like it would be in, say, storage.)\nBecause we will only access calldata through pointers, this offset is not that relevant, but it is worth noting.\nAlso note that in constructors, there is no 4-byte offset, but that\u0026rsquo;s because in constructors, calldata is empty (the special variable msg.sig is padded to contain 4 zero bytes). Parameters passed to constructors actually go in code rather than calldata \u0026ndash; and are represented the same way but with a different offset \u0026ndash; but since we will only deal with them once they have been copied onto the stack or into memory, we will ignore this.\n\nCalldata: Direct types and pointer types #  Direct types are the same as in memory. Nothing more needs to be said. Pointers to calldata are a bit different from pointers to memory, but you can see below about that.\n\nCalldata: Multivalue and lookup types (reference types) #  In order to understand reference types in calldata, we need the distinction of static and dynamic types that was introduced earlier.\nWith that in hand, then, variables of reference type in calldata are stored similarly to in memory (1, 2), with the difference that any of their elements of static reference type are not stored as pointers, but are simply stored inline; so unlike in memory, elements may take up multiple words. Elements of dynamic type are still stored as pointers (but see the section below about how those work).\nAlso, structs that contain mappings (or arrays of such) are entirely illegal in calldata, unlike in memory where the mappings are simply omitted.\nRemark: Calldata variables were only introduced in Solidity 0.5.0, so it is impossible to have variables of zero-element multivalue type in calldata; however, it still may be worth noting for other purposes that in the underlying encoding, such variables are omitted entirely in calldata (unlike in storage where they still take up a single word, or memory where it varies).\n\nThe special variable msg.data #  While I\u0026rsquo;ve thus far avoided discussing special variables, it\u0026rsquo;s worth pausing here to discuss the special variable msg.data, the one special variable of reference type. It is a bytes calldata. But it\u0026rsquo;s not represented like other variables of type bytes calldata, is it? It\u0026rsquo;s not some location in calldata with the number of bytes followed by the string of bytes; it simply is all of calldata. Accesses to it are simply accesses to the string of bytes that is calldata.\nThis raises the question: Given that calldata is of variable length, where is the length of msg.data stored? The answer, of course, is that this length is what is returned by the CALLDATASIZE instruction. This instruction could be considered something of a special location, and indeed many of the Solidity language\u0026rsquo;s special globally available variables are \u0026ldquo;stored\u0026rdquo; in such special locations, each with their own EVM opcode.\nWe have thus far ignored these special locations here and how they are encoded. However, since the variables kept in these other special locations are all of type uint256, address, or address payable; these special locations are word-based rather than byte-based (to the extent that distinction is meaningful here); and values from these special locations will always be copied to the (also word-based) stack before use, there is little to say about encoding in these special locations. One could say that addresses are, as always, zero-padded on the left, and that integers are, as always, stored in binary; and these statements would be true in a sense, but also largely meaningless.\nAnyway, none of this is really relevant here, so let\u0026rsquo;s move on from this digression and discuss pointers to calldata.\n\nPointers to calldata #  Pointers to calldata are different depending on whether they are from calldata or from the stack; and pointers to calldata from the stack are different depending on whether they point to a multivalue type or to a lookup type.\nNote, by the way, that there is no need for any sort of null pointer in calldata, and so no equivalent exists. (Variables in calldata of lookup type may be empty, of course, but distinct empty calldata variables are kept separate from another, not coalesced into a single null location like in memory.)\n\nPointers to calldata from calldata #  Pointers to calldata from calldata are relative, though in a slightly unusual manner. They are also given in bytes, but are relative not to the current location, but rather to the structure they are a part of (since they never stand alone.)\nFor pointers to calldata stored in variables of multivalue type, the pointer is relative to the start of that containing variable.\nFor pointers to calldata stored in variables of lookup type, the pointer is relative to the start of the list of elements, i.e., the word after the length.\nOr, to put it differently, either way it is always relative to the start of the list of elements it is contained in.\nNote that pointers to calldata from calldata will always be multiples of 0x20, since calldata, like memory, is padded (and these pointers are relative rather than absolute).\n\nPointers to calldata from the stack #  Pointers to a calldata multivalue types from the stack work just like pointers to memory: They are absolute, given in bytes, and always point to the start of a word. In calldata, though, the start of a word is congruent to 0x4 modulo 0x20, rather than being a multiple of 0x20.\nPointers to calldata lookup types from the stack take up two words on the stack rather than just one. The bottom word is a pointer \u0026ndash; absolute and given in bytes \u0026ndash; but points not to the word containing the length, but rather the start of the content, i.e., the word after the length (as described in the section on lookup types in memory), since lookup types in calldata are similar). The top word contains the length. Note, obviously, that if the length is zero then the value of the pointer is irrelevant (and the word it points to may contain unrelated data).\n\nStorage in detail #  [ ∧ Back to Locations in Detail ]\nStorage, unlike the other locations mentioned thus far, is a packed, not padded, location. The sizes in bytes of the direct types can be found in the direct types table.\nStorage is the one location other than the stack where we sometimes access variables directly rather than through pointers, so we will begin by describing data layout in storage.\n\nStorage: Data layout #  Storage is used to hold all state variables that are not declared constant or immutable. In what follows, we ignore constant and immutable variables, and look just at the ordinary state variables. (Variables declared constant are optimized out by the compiler; variables declared immutable are stored in code or memory instead.)\nFirst, we consider the case of a contract that does not inherit from any others.\nIn this case, state variables in storage are always laid out in the order that they were declared, starting from the beginning of storage. However, within a word, variables are laid out from right to left, not left to right (with one sort-of-exception to be described later). Variables of direct type may not cross a word boundary; if there is not enough room left at the top of a word for what comes next, the unused space at the top of the word remains filled with zeroes, and the next variable starts at the bottom of the next word.\nNote that this right-to-left orientation does not mean that the representations of direct types themselves are in any way reversed, only the order they\u0026rsquo;re laid out in within a word.\nVaiables of lookup type are, for this purpose, regarded as taking up one word; see the subsection on lookup types for more information.\nVariables of multivalue type must start on a word boundary, and always occupy whole words (i.e. the next variable after must start on a word boundary).\nAs mentioned above, variables declared constant or immutable are skipped.\nSubject to the above restrictions, every variable is placed as early as possible.\nNow, we consider inheritance.\nIn cases of inheritance, the variables of the base class go before those of the derived class. Note that there is not any sort of barrier between the variables of the base class and those of the derived class; variables of the base class and variables of the derived class may share a slot (so the first variable of the derived class need not start on a slot boundary).\nIn cases of multiple inheritance, Solidity uses the C3 linearization to order classes from \u0026ldquo;most base\u0026rdquo; to \u0026ldquo;most derived\u0026rdquo;, and then, as mentioned above, lays out variables starting with the most base and ending with the most derived. (Remember that, when listing parent classes, Solidity considers parents listed first to be \u0026ldquo;more base\u0026rdquo;; as the Solidity docs note, this is the reverse order from, say, Python.)\n\nStorage: Direct types #  The layout of direct types has already been described above, and the sizes of the direct types are found in the direct types table. Note that there are no pointer types in storage.\nNote that in Solidity 0.8.8, there was a bug that caused user-defined value types to always take up a full word in storage, regardless of the size of the underlying type; values of these types would be padded as normal.\n\nStorage: Multivalue types #  Variables of multivalue type simply have the elements stored consecutively within storage \u0026ndash; they are packed within the multivalue type just as variables are packed within storage. The rules are exactly the same.\nThe one exceptions is that (in pre-0.5.0 versions of Solidity where this was legal) multivalue types with zero elements still take up a single word, rather than zero words. (So, for instance, a uint[2][0] takes up 1 word, and a bytes1[0][3] takes up 3 words.)\nAgain, remember that variables of multivalue type must occupy whole words; they start on a word boundary, and whatever comes after starts on a word boundary too. And, obviously, this applies to variables of multivalue type within another variable of multivalue type since, as mentioned, the rules are exactly the same. (But I thought that case was worth highlighting.)\n\nStorage: Lookup types #  There are three lookup types that can go in storage: type[], bytes (and string, but again we will not treat that separately), and mapping(keyType =\u0026gt; elementType).\nAs mentioned above, we regard each lookup type as taking up one word; we will call this the \u0026ldquo;main word\u0026rdquo;.\nFor type[], i.e. an array, the main word contains the length of the array. Suppose the main word is in slot p and the length it contains is n. Then the array itself is stored exactly as if it were an array of type type[n] (see section above), except that it starts in the slot keccak256(p).\n(Yes, that is the position being used; no explicit pointer to the array location is stored. Other lookup types will be similar in this regard.)\nFor bytes, if the length (call it n) is less than 32, the low byte of the main word contains n\u0026lt;\u0026lt;1, and the string of bytes itself is stored in the same word, in sequence from left to right; any unused space within the word is left as zero.\nIf, on the other hand, the length n is at least 32, then the low byte of the main word contains (n\u0026lt;\u0026lt;1)|0x1, and the string of bytes is stored starting in the slot keccak256(p), where p is the position of the main word. This bytestring, too, goes from left to right within words, but (like an array) can take up as many words as necessary. Again, any unused space left within the last word is left as zero.\n(Type bytes (and string) is the one sort-of-exception I mentioned to the right-to-left rule within storage.)\nFinally, we have mappings. For mappings, the main word itself is unused and left as zero; only its position p is used. Mappings, famously, do not store what keys exist; keys that don\u0026rsquo;t exist and keys whose corresponding element is 0 (which is always the encoding of the default value (1, 2, 3, 4, 5) for anything in storage) are treated the same.\nFor a mapping map and a key key, then, the element map[key] is stored starting at keccak256(key . p), where . represents concatenation and key here has been converted to a string of bytes \u0026ndash; something that is meaningful for every key type. For the key types which are direct, the padded form is used; the value can be converted to a string of bytes by the representations listed in the section on direct types, with the padding as listed in the direct types table; for the lookup key type bytes (and string), well, this by itself represents a string of bytes! (No padding is applied to these.) Similarly, the position p is regarded as a 32-byte unsigned integer, because that is how storage locations are accessed.\nNote that if an element of a mapping is of direct type, this means it will always start on a slot boundary, even if it doesn\u0026rsquo;t normally have to. In any case, regardless of type, the element is stored exactly the same as it would be anywhere else in storage.\n\nPointers to storage #  Pointers to storage are absolute and are measured in words (slots), not bytes. (Such pointers are most easily regarded as pointing to the full slot, rather than a byte position within it; if you like, though, you can imagine it pointing to the latest position within the word, rather than the earliest, in accordance with the right-to-left nature of storage multivalue types, and the use of the low byte in types bytes and string.) This might seem to limit the locations such a pointer can point to; however, pointers to storage will always point to a variable of multivalue or lookup type, and such variables always start on a word boundary, so there is no problem here.\nNote that pointers to storage have a default value of 0x0. As stated earlier, one must beware \u0026ndash; making use of such a pointer can lead to nonsense! Such a pointer should not actually be used. (And note again that while it is legal to leave a storage pointer uninitialized, it is not legal to delete one.)\n ⚡  \nethdebug/solidity-data-representation\n"},{"id":10,"href":"/hub/docs/sushiswap/sushi-api/","title":"Sushi API","section":"Docs","content":"SushiSwap API #   API Endpoints for SushiSwap\n API #  ExchangeV1 #  /legacy/ API documentation\n   METHOD ENDPOINT     GET  /catchall+   GET  /legacy/summary    GET  /legacy/tickers    GET  /legacy/assets    GET  /legacy/orderbook/pair     GET  /legacy/trades/pair     Swap #  /swap/ API documentation\n   METHOD ENDPOINT      GET  /swap/summary    GET  /swap/tickers    GET  /swap/assets    GET  /swap/orderbook/pair    GET  /swap/trades/pair     License #  GPL-2.0\n"},{"id":11,"href":"/hub/docs/technical/katex/","title":"Katex","section":"Technical","content":"KaTeX #  KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nExample #  {{\u0026lt; katex [display] [class=\u0026#34;text-center\u0026#34;] \u0026gt;}} f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi {{\u0026lt; /katex \u0026gt;}}     \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\]    Display Mode Example #  Here is some inline example:  \\(\\pi(x)\\)  , rendered in the same line. And below is display example, having display: block  \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\]  Text continues here.\n"},{"id":12,"href":"/hub/docs/technical/mermaid/","title":"Mermaid","section":"Technical","content":"Mermaid Chart #  MermaidJS is library for generating svg charts and diagrams from text.\nOverride Mermaid Initialization Config\nTo override the initialization config for Mermaid, create a mermaid.json file in your assets folder!\n Example #  {{\u0026lt; mermaid [class=\u0026#34;text-center\u0026#34;]\u0026gt;}} stateDiagram-v2 State1: The state with a note note right of State1 Important information! You can write notes. end note State1 --\u0026gt; State2 note left of State2 : This is the note to the left. {{\u0026lt; /mermaid \u0026gt;}}    mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) stateDiagram-v2 State1: The state with a note note right of State1 Important information! You can write notes. end note State1 -- State2 note left of State2 : This is the note to the left.   "},{"id":13,"href":"/hub/posts/2021_10_25/","title":"2021 10 25","section":"Blog","content":"2021.10.25 - Community update #   New Developer and Community Hub https://manifoldfinance.github.io/hub/ Starting point and organizational checkpoint for providing updates and engaging with each other   Reduce the effort to get updates and find historical updates Reduce the \u0026lsquo;mission creep\u0026rsquo; that was happening (multiple different repos for documentation, hard to keep up if casual end user) Reduce effort of formulating and providing updates  Mitigating Security Issues #  In the course of working with the sushi team, we caught a \u0026lsquo;supply chain attack\u0026rsquo; npm package (this was actually apart of an unclaimed bug bounty) A repository is produced here: https://github.com/sambacha/openzeppelin-splinter\nThrough a mix of versioning package managers (yarn vs npm) a downstream dependency of the Sushiswap interface (also the Uniswap interface) could become compromised. The TLDR is:\n the package name includes the versioning. it correctly maps to the proper openzeppelin-solidity however the package name was not taken until months afterwards\n  \u0026ldquo;openzeppelin-solidity-2.3.0\u0026rdquo;: \u0026ldquo;npm:openzeppelin-solidity@2.3.0\u0026rdquo;\n OpenMEV #   currently onboarding 3-4 new searchers to participate in the sushi process with us released flashbots fork of mev-geth, as flashbots has decided to not support eth_callBundle RPC Method  docker container \u0026amp; go-ethereum MEV\n   How does OpenMEV\u0026rsquo;s infrastructure operate in suboptimal conditions? #  This will be expanded in an article, and if you have additional questions please ask in discord, the forums or telegram. The quick summary is like this:\nFor state and error recovery, we use a weight system similar to this: https://github.com/sambacha/web3-rpc-failover\nFor predictive analytics (i.e. knowing ahead of time that we may have issues soon) you can see this repo here that explores Littles Law and backpressure/data plane routing: https://github.com/sambacha/capacity . This is especially relevant towards end users as we measure how long it takes for a transaction to be mined vs. the parameters of the transaction when it was first sent. Here is some sample results for the simulation capacity repo to help you better understand what we are discussing:\nclient sends 5 requests per second and receives between 2.4 rps and 2.7 rps (HTTP 200) proxy oscillates between 4 and 6 in-flight requests origin processes between 2.7 and 3 requests per second origin has served requests with average latency 2.2 seconds origin has served 50% of requests (50th percentile) within 2 seconds origin has served 99% of requests (99th percentile) within 3 seconds with periodic spikes Contracts and Staking #  Contracts for deploying strategies and trades are in their preliminary acceptance phase, a.k.a Release Candidate. Contracts are located in the Manifold SDK monorepo here. The First deployed strategy will be the USD strategy for Fiat / Crypto On/Off ramp usage.\nStaking and DAO beta testing will be in 3 weeks.\nDeveloper and Searcher Services #  We are working on providing a custom plugin module system that will allow end users to load or provide a custom go module that enables new and unique RPC methods for their own private usage. We have had a few people ask about this idea i general and plan on supporting it. In addition users can purchase per RPC method requests to our service in a pay-for-what-you-use model. Pricing will be denominated in GxGas, the standard fixed unit of account of gwei. Gwei costs are paid via $FOLD.\n"},{"id":14,"href":"/hub/posts/daosclarosis/","title":"Daosclarosis","section":"Blog","content":"DAOsclarosis #  on the persistence of faulty models in governance #   “Demosclerosis isn’t a problem you solve It’s a problem you manage.”\nJonathan Rauch, DEMOSCLEROSIS\nThe Silent Killer of American Government, 1994\n The DAO Corollary #   F.K.A. Amdahl’s Corollary\n The most efficient way to implement a piece of software is to do it all yourself.\nNo time is wasted communicating (or arguing); everything that needs to be done is done by the same person, which increases their ability to maintain the software; and the code is by default way more consistent.\nTurns out “more efficient (alt effective)” doesn’t mean “faster (both in performance and time to delivery)”. When there are more people working on the same problem, we can parallelize more at once.\nWhen we break work up across a team, in order to optimize for the team, we often have to put more work in, individually, to ensure that the work can be efficiently parallelized. This includes explaining concepts, team meetings, code review, pair programming, etc. But by putting that work in, we make the work more parallelized, speeding up and allowing us to make greater gains in the future.\nAmdahl’s Law #  Amdahl’s law can be formulated as follows:\n$$\nS_{\\text {latency }}=\\frac{1}{(1-p)+\\frac{p}{s}}\n$$\nIn other words, it predicts the maximum potential speedup (Slatency), given a proportion of the task, p, that will benefit from improved (either more or better) resources, and a parallel speedup factor, s.\nTo demonstrate, if we can speed up 10% of the task (p=0.1) by a factor of 5 (s=5), we get the following:\n$$\nS_{\\text {latency }}=\\frac{1}{(1–0.1)+\\frac{0.1}{5}} \\approx 1.09\n$$\nThat’s about a 9% speedup — Acceptable.\nHowever, if we can speed up 90% of the task (p=0.9) by a factor of 5 (s=5), we get the following:\n$$\nS_{\\text {latency }}=\\frac{1}{(1–0.9)+\\frac{0.9}{5}} \\approx 3.58\n$$\nThat’s roughly a 250% increase! Big enough that it’s actually worth creating twice as much work; it still pays off, assuming the value of the work dwarfs the cost of the resources.\n$s \\rightarrow \\infty$, which means $\\frac{p}{s} \\rightarrow 0$, so we can also drop the $\\frac{p}{s}$ term if we can afford potentially infinite resources at no additional cost.\n$$\nS_{\\text {latency }}=\\frac{1}{1–0.9}=10\n$$\nIn other words, if 90% of the work can be parallelised, we can achieve a theoretical maximum speedup of 10x, or a 900% increase. This is highly unlikely, but gives us a useful upper bound to help us identify where the bottleneck lies.\nGeneralizing a PID to the amount of work #  Typically, we start off with a completely serial process. In order to parallelize, we need to do more work. It doesn’t come for free.\nThis means that when computing $s$, the parallel speedup, we should divide it by the cost of parallelization.\nFor example, if the cost is 2, that means that making the work parallelisable (without actually increasing the number of resources) makes the parallel portion take twice as long as it used to. (The serial portion is unchanged.)\nSo, if we take the example from earlier, where 90% of the work is parallelisable but it costs twice as much to parallelized, we’ll get the following result:\n$$\nS_{\\text {latency }}=\\frac{1}{(1–0.9)+\\frac{0.9}{\\frac{5}{2}}} \\approx 2.18\n$$\nIt’s still about a $117 %$ increase in output!\nHowever, if $p=0.1$, then there’s really very little point in adding more resources.\n$$\nS_{\\text {latency }}=\\frac{1}{(1–0.1)+\\frac{0.1}{\\frac{5}{2}}} \\approx 1.06\n$$\nAnd if the cost of parallelisation is greater than the potential speedup, bad things happen:\n$$\nS_{\\text {latency }}=\\frac{1}{(1–0.1)+\\frac{0.1}{\\frac{5}{20}}} \\approx 0.769\n$$\nAdding 4 more resources slows us down by 23%. Many of us have seen this happen in practice with poor parallelization techniques — poor usage of locks, resource contention (especially with regards to I/O), or even redundant work due to mismanaged job distribution.\nSo, What Does It All Mean? #  Amdahl’s law tells us something very insightful:\n When the value of your work is much greater than the cost, you should optimize for parallelism, not efficiency.\n The cost of a weekly two-hour team meeting is high (typically in the $1000s each time), but if it means that you can have 7 people on the team, not 3, it’s often worth it.\nDelivering faster means you can deliver more.\nBetter to have 10 people working on 5 problems and doing a better job than it is to have 10 people working on 10 problems.\nThe former will lead to fewer conflicts, fewer defects and a much more motivated team. I.e. $p$ and $s$ produce greater returns, faster than the amount of work.\nConversely, if all the knowledge of how the product works is in one person’s head, $p≈0$. While there’s no impact to efficiency this way, it limits our ability to produce, because one person can only do so much. Adding more people just makes things slower.\n"},{"id":15,"href":"/hub/posts/promise_monad/","title":"Promise Monad","section":"Blog","content":"async/await is just the do-notation of the Promise monad #   source gist\n CertSimple just wrote a blog post arguing ES2017\u0026rsquo;s async/await was the best thing to happen with JavaScript. I wholeheartedly agree.\nIn short, one of the (few?) good things about JavaScript used to be how well it handled asynchronous requests. This was mostly thanks to its Scheme-inherited implementation of functions and closures. That, though, was also one of its worst faults, because it led to the \u0026ldquo;callback hell\u0026rdquo;, an seemingly unavoidable pattern that made highly asynchronous JS code almost unreadable. Many solutions attempted to solve that, but most failed. Promises almost did it, but failed too. Finally, async/await is here and, combined with Promises, it solves the problem for good. On this post, I\u0026rsquo;ll explain why that is the case and trace a link between promises, async/await, the do-notation and monads.\nFirst, let\u0026rsquo;s illustrate the 3 styles by implementing a function that returns the balances of all your Ethereum accounts.\nWrong solution: callback hell #  function getBalances(callback) { web3.eth.accounts(function (err, accounts) { if (err) { callback(err); } else { var balances = {}; var balancesCount = 0; accounts.forEach(function(account, i) { web3.eth.getbalance(function (err, balance) { if (err) { callback(err); } else { balances[account] = balance; if (++balancesCount === accounts.length) { callback(null, balances); } } }); }); } }); }; The earliest way to solve this problem was to use callbacks, which caused the dreaded \u0026ldquo;callback hell\u0026rdquo;, evident from the ugliness of the code above. There are 3 major issues to blame:\n  Explicit error propagation;\n  Keeping track of multiple async values with a counter;\n  Unavoidable nesting.\n  Almost correct solution: Promises #  function getBalances() { return web3.eth.accounts() .then(accounts =\u0026gt; Promise.all(accounts.map(web3.eth.getBalance)) .then(balances =\u0026gt; Ramda.zipObject(accounts, balances))); } Promises are first-class terms representing future values. They can be created at will, chained and returned from functions. They almost solve the 3 problems above.\n  Errors are propagated automatically through .then() chains;\n  Promise.all() tracks multiple async values cleanly;\n  Nesting is almost always avoidable by clever usage of .then().\n  There is one leftover problem, though: .then() still requires nesting in some cases. On my example, for one, the second .then() had to be inside the first, otherwise accounts wouldn\u0026rsquo;t be in scope. This forced me to indent the code right. This situation can sometimes be fixed by passing the value ahead:\nfunction getBalances() { return web3.eth.accounts() .then(accounts =\u0026gt; Promise.all(accounts.map(web3.eth.getBalance).then(balances =\u0026gt; [accounts, balances]))) .then(([accounts, balances]) =\u0026gt; Ramda.zipObject(accounts, balances)); } But that is verbose and, as such, won\u0026rsquo;t work if you have a complex dependency tree, forcing you to indent everything again. In short, Promises solve the callback hell for most cases, but not all.\nCorrect solution: async/await #  async function getBalances() { const accounts = await web3.eth.accounts(); const balances = await Promise.all(accounts.map(web3.eth.getBalance)); return Ramda.zipObject(balances, accounts); } The new async functions allow you to use await, which causes the function to wait for the promise to resolve before continuing its execution. With that, we\u0026rsquo;re able to solve the indenting problem once for all, for any arbitrary code. Now, I\u0026rsquo;ll explain why that is the case. Before, though, let\u0026rsquo;s talk about the actual origin of the problem.\nThe origins of the callback hell #  Many associate the \u0026ldquo;callback hell\u0026rdquo; with asynchronous values, but the problem is much more widespread than that: it naturally arises anytime computation must be performed on \u0026ldquo;wrapped\u0026rdquo; values that are accessed through callbacks. For a simple example, suppose that you wanted to print all the combinations of numbers from the arrays [1,2,3], [4,5,6], [7,8,9]. This is a way to do it:\n[1,2,3].map((x) =\u0026gt; { [4,5,6].map((y) =\u0026gt; { [7,8,9].map((z) =\u0026gt; { console.log(x,y,z); }) }) }); Notice how the familiar nesting shows up here: it is caused by the same inherent issue that causes the callback hell. Compare [1,2,3].map(x =\u0026gt; ...) to promise.then(x =\u0026gt; ...). If mapping over nested arrays was as common as dealing with asynchronous values, by 2018 we\u0026rsquo;d probably have an multi/pick syntax:\nmulti function () { x = pick [1, 2, 3]; y = pick [4, 5, 6]; z = pick [7, 8, 9]; console.log(x, y, z); } \u0026hellip; which is exactly equivalent to async/await, except that, instead of implementing terms that can be defined in a future, it implements terms that can have multiple values simultaneously.\nMonads and do-notation #  In some languages such as Haskell and Idris, values which can be wrapped and chained in arbitrary ways are much more common. As such, they had to develop a way to deal with all of them, not just a specific case. Their solution was to specify an \u0026ldquo;interface\u0026rdquo; for wrappeable/chainable values, and a special syntax to flatten arbitrary chains of wrapped values.\n// Pseudo-code of an interface for wrappeable/chainable values. // `wrap` receives a value and wraps it. // `chain` receives a wrapped value, a callback and returns a wrapped value. interface Wrapper\u0026lt;W\u0026gt; { wrap\u0026lt;A\u0026gt;(value:A): W\u0026lt;A\u0026gt;, chain\u0026lt;A,B\u0026gt;(value:W\u0026lt;A\u0026gt;, callback: A =\u0026gt; W\u0026lt;B\u0026gt;): W\u0026lt;B\u0026gt; } The syntax was called the do-notation, and the interface was named \u0026ldquo;Monad\u0026rdquo;. Thanks those 2 insights, they have, since 1995, been free from their \u0026ldquo;callback hell\u0026rdquo; problem. That is why JavaScript finally did it too. If you squint a little bit, JS Promises are just monads of future values that can throw: Promise.resolve(value) implements wrap, and value.then(callback) implements chain. Under that point of view, async/await is just the do-notation for that specific monad, providing the same kind of arbitrary chain flattening. It is, ultimately, the same solution and, given that it worked so well on Haskell, it is reasonable to assume it will work on JavaScript too.\n-- Monad is just a fancy name for the Wrapper interface above class Monad m where return :: a -\u0026gt; m a -- wrap (\u0026gt;\u0026gt;=) :: m a -\u0026gt; (a -\u0026gt; m b) -\u0026gt; m b -- chain -- Promises are monads, so we can make an instance for it instance Monad Promise where return value = Promise.resolve value value \u0026gt;\u0026gt;= callback = Promise.then value callback -- Then the do-syntax becomes equivalent to async/await getBalances :: Promise (Map String String) getBalances = do accounts \u0026lt;- getAccounts balances \u0026lt;- getBalance accounts return (Map.fromList (zip accounts balances)) Now everything is good and JavaScript is great again, yet I wonder why it took so long. If only the community listened that crazy guy about 5 years ago\u0026hellip;\n"},{"id":16,"href":"/hub/docs/sushiswap/","title":"OpenMEV Documentation","section":"Docs","content":"OpenMEV \u0026amp; Sushiswap #  OpenMEV and Sushiswap work together to recapture lost MEV profits so that Sushi traders can trade for free (up to 95%) on the exchange.\nThese are not gasless transactions, these are paid rebate transactions! You get paid to trade more using OpenMEV.\nSupport and Help #  !!! note We will never ask you to send tokens or coins anywhere!\n Support Page Discord Chat  Trading for Free on Sushi.com #    baseFee amount is subject to transaction cost rebates\n  Transactions with a slippage tolerance of 1% or higher are eligible for gas refunds\n  Refunds are paid out in ETH/WETH/xSUSHI\n  It will take at least 35 blocks to receive a refund, no more than 50.\n  Getting Gas Pricing information #  Our API Endpoint is here\ncurl -s -L api.txprice.com | jq .blockPrices[0].estimatedPrices[3] { \u0026#34;confidence\u0026#34;: 80, \u0026#34;price\u0026#34;: 112, \u0026#34;maxPriorityFeePerGas\u0026#34;: 1.76, \u0026#34;maxFeePerGas\u0026#34;: 222.52 } Engine #  OpenMEV Engine uses a batch auction-based matching engine to execute orders. Batch auctions were chosen to reduce the impact of frontrunning on the exchange.\n  All orders for the given market are collected.\n  Orders beyond their time-in-force are canceled.\n  Orders are placed into separate lists by market side, and aggregate supply and demand curves are calculated.\n  The matching engine discovers the price at which the aggregate supply and demand curves cross, which yields the clearing price. If there is a horizontal cross - i.e., two prices for which aggregate supply and demand are equal - then the clearing price is the midpoint between the two prices.\n  If both sides of the market have equal volume, then all orders are completely filled. If one side has more volume than the other, then the side with higher volume is rationed pro-rata based on how much its volume exceeds the other side. For example, if aggregate demand is 100 and aggregate supply is 90, then every order on the demand side of the market will be matched by 90%.\n  Orders are sorted based on their price, and order ID. Order IDs are generated at post time and is the only part of the matching engine that is time-dependent. However, the oldest order IDs are matched first so there is no incentive to post an order ahead of someone else’s.\n"}]